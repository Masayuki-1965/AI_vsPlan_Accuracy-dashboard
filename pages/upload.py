import streamlit as st
import pandas as pd
import numpy as np
import chardet
from utils.validators import validate_data, validate_required_columns
from utils.data_processor import (
    preview_data, 
    calculate_abc_classification, 
    validate_abc_categories, 
    get_abc_classification_summary
)
from config.settings import ABC_CLASSIFICATION_SETTINGS

def show():
    """ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
    st.header("ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'original_data' not in st.session_state:
        st.session_state.original_data = None
    if 'uploaded_filename' not in st.session_state:
        st.session_state.uploaded_filename = None
    if 'data_columns' not in st.session_state:
        st.session_state.data_columns = []
    if 'current_mapping' not in st.session_state:
        st.session_state.current_mapping = {}
    if 'encoding_info' not in st.session_state:
        st.session_state.encoding_info = None
    if 'mapping_completed' not in st.session_state:
        st.session_state.mapping_completed = False
    if 'abc_categories' not in st.session_state:
        st.session_state.abc_categories = ABC_CLASSIFICATION_SETTINGS['default_categories'].copy()
    if 'abc_auto_generate' not in st.session_state:
        st.session_state.abc_auto_generate = True
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.subheader("1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
        type=['csv'],
        help="åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆ
    if uploaded_file is not None:
        # ãƒ•ã‚¡ã‚¤ãƒ«åãŒå¤‰ã‚ã£ãŸå ´åˆã®ã¿å‡¦ç†
        if st.session_state.uploaded_filename != uploaded_file.name:
            try:
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤åˆ¥ï¼‰
                df, encoding_info = read_csv_with_encoding(uploaded_file)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state.original_data = df
                st.session_state.uploaded_filename = uploaded_file.name
                st.session_state.data_columns = list(df.columns)
                st.session_state.encoding_info = encoding_info
                st.session_state.current_mapping = {}
                st.session_state.mapping_completed = False
                
                st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
                
            except Exception as e:
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                return
    
    # ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®è¡¨ç¤º
    if st.session_state.original_data is not None:
        df = st.session_state.original_data
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
        if st.session_state.encoding_info:
            st.info(f"ğŸ“ èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {st.session_state.uploaded_filename}")
            st.info(f"ğŸ” {st.session_state.encoding_info}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        st.subheader("2. ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(df.head(10), use_container_width=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°
        st.subheader("3. ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®š")
        st.info("CSVã®ã‚«ãƒ©ãƒ åã‚’ã‚·ã‚¹ãƒ†ãƒ é …ç›®ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦ãã ã•ã„")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®šUI
        mapping = {}
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**å¿…é ˆé …ç›®**")
            mapping['P_code'] = st.selectbox(
                "å•†å“ã‚³ãƒ¼ãƒ‰",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('P_code', '')),
                help="å•†å“ã‚’è­˜åˆ¥ã™ã‚‹ã‚³ãƒ¼ãƒ‰ï¼ˆå¿…é ˆï¼‰"
            )
            mapping['Date'] = st.selectbox(
                "å¹´æœˆ",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Date', '')),
                help="YYYYMMå½¢å¼ã®å¹´æœˆãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰"
            )
            mapping['Actual'] = st.selectbox(
                "å®Ÿç¸¾å€¤",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Actual', '')),
                help="å®Ÿéš›ã®å£²ä¸Šãƒ»éœ€è¦å®Ÿç¸¾ï¼ˆå¿…é ˆï¼‰"
            )
            mapping['AI_pred'] = st.selectbox(
                "AIäºˆæ¸¬å€¤",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('AI_pred', '')),
                help="AIã«ã‚ˆã‚‹äºˆæ¸¬å€¤ï¼ˆå¿…é ˆï¼‰"
            )
            mapping['Plan_01'] = st.selectbox(
                "è¨ˆç”»å€¤01",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Plan_01', '')),
                help="åŸºæº–ã¨ãªã‚‹è¨ˆç”»å€¤ï¼ˆå¿…é ˆï¼‰"
            )
            
            # ABCåŒºåˆ†ã‚’å¿…é ˆé …ç›®ã¨ã—ã¦è¿½åŠ 
            st.markdown("**ABCåŒºåˆ†è¨­å®šï¼ˆå¿…é ˆï¼‰**")
            mapping['Class_abc'] = st.selectbox(
                "ABCåŒºåˆ†ã‚«ãƒ©ãƒ ",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Class_abc', '')),
                help="CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ABCåŒºåˆ†ã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯é¸æŠã—ã¦ãã ã•ã„"
            )
            
            # ABCåŒºåˆ†ã®è‡ªå‹•ç”Ÿæˆè¨­å®š
            abc_has_column = bool(mapping['Class_abc'])
            if not abc_has_column:
                st.info("ğŸ’¡ ABCåŒºåˆ†ã‚«ãƒ©ãƒ ãŒé¸æŠã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å®Ÿç¸¾å€¤ã«åŸºã¥ã„ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™")
                st.session_state.abc_auto_generate = True
            else:
                st.session_state.abc_auto_generate = st.checkbox(
                    "ABCåŒºåˆ†ã‚’è‡ªå‹•ç”Ÿæˆã§ä¸Šæ›¸ãã™ã‚‹", 
                    value=False,
                    help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã€CSVã®ABCåŒºåˆ†ã‚’ç„¡è¦–ã—ã¦å®Ÿç¸¾å€¤ã‹ã‚‰è‡ªå‹•è¨ˆç®—ã—ã¾ã™"
                )
            
        with col2:
            st.markdown("**ä»»æ„é …ç›®**")
            mapping['Class_01'] = st.selectbox(
                "åˆ†é¡01",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Class_01', '')),
                help="å•†å“åˆ†é¡ãƒ»ã‚«ãƒ†ã‚´ãƒª1ï¼ˆä»»æ„ï¼‰"
            )
            mapping['Class_02'] = st.selectbox(
                "åˆ†é¡02",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Class_02', '')),
                help="å•†å“åˆ†é¡ãƒ»ã‚«ãƒ†ã‚´ãƒª2ï¼ˆä»»æ„ï¼‰"
            )
            mapping['Plan_02'] = st.selectbox(
                "è¨ˆç”»å€¤02",
                options=[''] + st.session_state.data_columns,
                index=get_selectbox_index(st.session_state.data_columns, st.session_state.current_mapping.get('Plan_02', '')),
                help="æ¯”è¼ƒç”¨ã®è¨ˆç”»å€¤ï¼ˆä»»æ„ï¼‰"
            )
        
        # ç¾åœ¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’æ›´æ–°
        st.session_state.current_mapping = mapping
        
        # ABCåŒºåˆ†è¨­å®šã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        with st.expander("ğŸ”§ ABCåŒºåˆ†è‡ªå‹•ç”Ÿæˆè¨­å®š", expanded=st.session_state.abc_auto_generate):
                # ç¾åœ¨ã®çŠ¶æ…‹è¡¨ç¤º
                if st.session_state.abc_auto_generate:
                    st.success("ğŸŸ¢ **è‡ªå‹•ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰**: å®Ÿç¸¾å€¤ã«åŸºã¥ã„ã¦ABCåŒºåˆ†ã‚’è‡ªå‹•è¨ˆç®—ã—ã¾ã™")
                else:
                    st.info("ğŸŸ¡ **æ‰‹å‹•æŒ‡å®šãƒ¢ãƒ¼ãƒ‰**: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ABCåŒºåˆ†ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™")
                
                st.markdown("### è‡ªå‹•ç”Ÿæˆæ™‚ã®åŒºåˆ†è¨­å®š")
                st.markdown("å®Ÿç¸¾å€¤ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆã—ã€ç´¯ç©æ§‹æˆæ¯”ã«åŸºã¥ã„ã¦ä»¥ä¸‹ã®åŒºåˆ†ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ï¼š")
                
                # ç¾åœ¨ã®åŒºåˆ†è¨­å®šã‚’è¡¨ç¤ºãƒ»ç·¨é›†
                categories_df = pd.DataFrame(st.session_state.abc_categories)
                
                # åŒºåˆ†ã®è¿½åŠ 
                col1, col2 = st.columns(2)
                with col1:
                    new_category_name = st.selectbox(
                        "è¿½åŠ ã™ã‚‹åŒºåˆ†",
                        options=[''] + [cat['name'] for cat in ABC_CLASSIFICATION_SETTINGS['additional_categories']],
                        help="D, E, F, G, H, ZåŒºåˆ†ã‚’è¿½åŠ ã§ãã¾ã™"
                    )
                with col2:
                    if new_category_name and st.button("åŒºåˆ†ã‚’è¿½åŠ "):
                        # æ—¢å­˜ã®åŒºåˆ†åã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        existing_names = [cat['name'] for cat in st.session_state.abc_categories]
                        if new_category_name not in existing_names:
                            # æ–°ã—ã„åŒºåˆ†ã‚’æœ«å°¾ã«è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¯„å›²ã¯æœ€å¾Œã®åŒºåˆ†ã®å¾Œï¼‰
                            last_end = max([cat['end_ratio'] for cat in st.session_state.abc_categories]) if st.session_state.abc_categories else 0.0
                            new_category = {
                                'name': new_category_name,
                                'start_ratio': last_end,
                                'end_ratio': min(1.0, last_end + 0.1),
                                'description': f'{new_category_name}åŒºåˆ†'
                            }
                            st.session_state.abc_categories.append(new_category)
                            st.rerun()
                        else:
                            st.warning(f"åŒºåˆ† '{new_category_name}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")
                
                # åŒºåˆ†è¨­å®šã®ç·¨é›†
                st.markdown("### æ§‹æˆæ¯”ç¯„å›²è¨­å®š")
                edited_categories = []
                
                for i, category in enumerate(st.session_state.abc_categories):
                    col1, col2, col3, col4 = st.columns([1, 2, 2, 1])
                    
                    with col1:
                        st.text(category['name'])
                    
                    with col2:
                        start_ratio = st.number_input(
                            f"é–‹å§‹%",
                            min_value=0.0,
                            max_value=100.0,
                            value=category['start_ratio'] * 100,
                            step=1.0,
                            key=f"start_{i}",
                            help="ã“ã®åŒºåˆ†ã®é–‹å§‹æ§‹æˆæ¯”ï¼ˆ%ï¼‰"
                        ) / 100.0
                    
                    with col3:
                        end_ratio = st.number_input(
                            f"çµ‚äº†%",
                            min_value=0.0,
                            max_value=100.0,
                            value=category['end_ratio'] * 100,
                            step=1.0,
                            key=f"end_{i}",
                            help="ã“ã®åŒºåˆ†ã®çµ‚äº†æ§‹æˆæ¯”ï¼ˆ%ï¼‰"
                        ) / 100.0
                    
                    with col4:
                        if len(st.session_state.abc_categories) > 1:  # æœ€ä½1ã¤ã¯æ®‹ã™
                            if st.button("ğŸ—‘ï¸", key=f"delete_{i}", help="ã“ã®åŒºåˆ†ã‚’å‰Šé™¤"):
                                st.session_state.abc_categories.pop(i)
                                st.rerun()
                    
                    edited_categories.append({
                        'name': category['name'],
                        'start_ratio': start_ratio,
                        'end_ratio': end_ratio,
                        'description': category.get('description', f'{category["name"]}åŒºåˆ†')
                    })
                
                # è¨­å®šã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                is_valid, error_msg = validate_abc_categories(edited_categories)
                if not is_valid:
                    st.error(f"âŒ åŒºåˆ†è¨­å®šã‚¨ãƒ©ãƒ¼: {error_msg}")
                else:
                    st.session_state.abc_categories = edited_categories
                    st.success("âœ… åŒºåˆ†è¨­å®šãŒæœ‰åŠ¹ã§ã™")
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™ãƒœã‚¿ãƒ³
                if st.button("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã™"):
                    st.session_state.abc_categories = ABC_CLASSIFICATION_SETTINGS['default_categories'].copy()
                    st.rerun()
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèªãƒ»ä¿å­˜
        if st.button("ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®šã‚’é©ç”¨", type="primary"):
            # å¿…é ˆé …ç›®ãƒã‚§ãƒƒã‚¯ï¼ˆABCåŒºåˆ†ã¯é™¤ã - è‡ªå‹•ç”Ÿæˆã™ã‚‹ãŸã‚ï¼‰
            required_fields = ['P_code', 'Date', 'Actual', 'AI_pred', 'Plan_01']
            missing_fields = [field for field in required_fields if not mapping[field]]
            
            if missing_fields:
                st.error(f"âŒ å¿…é ˆé …ç›®ãŒæœªè¨­å®šã§ã™: {', '.join(missing_fields)}")
            else:
                try:
                    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
                    mapped_df = apply_mapping(df, mapping)
                    
                    # ABCåŒºåˆ†ã®å‡¦ç†
                    abc_needs_generation = st.session_state.abc_auto_generate or not mapping.get('Class_abc')
                    
                    if abc_needs_generation:
                        # ABCåŒºåˆ†ã‚’è‡ªå‹•ç”Ÿæˆ
                        with st.status("ğŸ”„ ABCåŒºåˆ†ã‚’è‡ªå‹•ç”Ÿæˆä¸­...", expanded=True) as status:
                            st.write("ğŸ“Š å®Ÿç¸¾å€¤ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...")
                            
                            # åŒºåˆ†è¨­å®šã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                            is_valid, error_msg = validate_abc_categories(st.session_state.abc_categories)
                            if not is_valid:
                                st.error(f"âŒ ABCåŒºåˆ†è¨­å®šã‚¨ãƒ©ãƒ¼: {error_msg}")
                                status.update(label="âŒ ABCåŒºåˆ†è¨­å®šã‚¨ãƒ©ãƒ¼", state="error")
                                return
                            
                            st.write("ğŸ”¢ å•†å“ã‚³ãƒ¼ãƒ‰åˆ¥å®Ÿç¸¾å€¤ã‚’é›†è¨ˆä¸­...")
                            
                            # ABCåŒºåˆ†ã‚’è¨ˆç®—
                            try:
                                mapped_df = calculate_abc_classification(
                                    mapped_df, 
                                    categories=st.session_state.abc_categories,
                                    base_column='Actual'
                                )
                                st.write("âœ… ABCåŒºåˆ†ã®å‰²ã‚Šå½“ã¦å®Œäº†")
                                
                                # ç”Ÿæˆçµæœã®è¡¨ç¤º
                                abc_summary = get_abc_classification_summary(mapped_df, 'Class_abc', 'Actual')
                                if abc_summary:
                                    st.write("ğŸ“ˆ é›†è¨ˆçµæœ:")
                                    
                                    # å„åŒºåˆ†ã®è©³ç´°æƒ…å ±
                                    for category in sorted(st.session_state.abc_categories, key=lambda x: x['start_ratio']):
                                        cat_name = category['name']
                                        count = abc_summary['counts'].get(cat_name, 0)
                                        ratio = abc_summary['ratios'].get(cat_name, 0)
                                        range_text = f"{category['start_ratio']*100:.0f}%-{category['end_ratio']*100:.0f}%"
                                        st.write(f"ã€€â€¢ {cat_name}åŒºåˆ†({range_text}): {count}ä»¶ ({ratio:.1f}%)")
                                    
                                    status.update(label="âœ… ABCåŒºåˆ†è‡ªå‹•ç”Ÿæˆå®Œäº†", state="complete")
                                else:
                                    st.warning("âš ï¸ ABCåŒºåˆ†ã®é›†è¨ˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
                                    
                            except Exception as e:
                                st.error(f"âŒ ABCåŒºåˆ†è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                status.update(label="âŒ ABCåŒºåˆ†è¨ˆç®—ã‚¨ãƒ©ãƒ¼", state="error")
                                return
                    else:
                        st.info("ğŸ“‹ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ABCåŒºåˆ†ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™")
                    
                    # åŸºæœ¬æ¤œè¨¼
                    if validate_mapped_data(mapped_df):
                        st.session_state.data = mapped_df
                        st.session_state.mapping = mapping
                        st.session_state.mapping_completed = True
                        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†ï¼ä»–ã®ãƒšãƒ¼ã‚¸ã§åˆ†æã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
                        st.rerun()
                    else:
                        st.error("âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                        
                except Exception as e:
                    st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†å¾Œã®è¡¨ç¤º
        if st.session_state.mapping_completed and st.session_state.data is not None:
            # ãƒãƒƒãƒ”ãƒ³ã‚°çµæœè¡¨ç¤º
            st.subheader("4. ãƒãƒƒãƒ”ãƒ³ã‚°çµæœ")
            mapping_df = pd.DataFrame([
                {"ã‚·ã‚¹ãƒ†ãƒ é …ç›®": k, "CSVã‚«ãƒ©ãƒ ": v, "ãƒ‡ãƒ¼ã‚¿å‹": str(st.session_state.data[k].dtype) if v else "æœªè¨­å®š"}
                for k, v in st.session_state.current_mapping.items() if v
            ])
            st.dataframe(mapping_df, use_container_width=True)
            
            # å¤‰æ›å¾Œãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            st.subheader("5. å¤‰æ›å¾Œãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(st.session_state.data.head(), use_container_width=True)
            
            # ABCåŒºåˆ†ã®é›†è¨ˆçµæœè¡¨ç¤º
            if 'Class_abc' in st.session_state.data.columns:
                st.subheader("6. ABCåŒºåˆ†é›†è¨ˆçµæœ")
                abc_summary = get_abc_classification_summary(st.session_state.data, 'Class_abc', 'Actual')
                
                if abc_summary:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**ä»¶æ•°åˆ†å¸ƒ**")
                        counts_df = pd.DataFrame(list(abc_summary['counts'].items()), 
                                                columns=['ABCåŒºåˆ†', 'ä»¶æ•°'])
                        st.dataframe(counts_df, use_container_width=True)
                    
                    with col2:
                        if 'ratios' in abc_summary:
                            st.markdown("**å®Ÿç¸¾å€¤æ§‹æˆæ¯”**")
                            ratios_df = pd.DataFrame(list(abc_summary['ratios'].items()), 
                                                    columns=['ABCåŒºåˆ†', 'æ§‹æˆæ¯”(%)'])
                            st.dataframe(ratios_df, use_container_width=True)
    
    # æ—¢ã«ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®æƒ…å ±è¡¨ç¤º
    if st.session_state.data is not None:
        st.subheader("ğŸ’¾ èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±")
        df = st.session_state.data
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ãƒ‡ãƒ¼ã‚¿ä»¶æ•°", len(df))
        with col2:
            st.metric("æœŸé–“ç¯„å›²", f"{df['Date'].min()} - {df['Date'].max()}")
        with col3:
            st.metric("å•†å“ã‚³ãƒ¼ãƒ‰æ•°", df['P_code'].nunique())

def get_selectbox_index(options, value):
    """selectboxã®indexå€¤ã‚’å–å¾—"""
    try:
        if value in options:
            return options.index(value) + 1  # ç©ºã®é¸æŠè‚¢ãŒã‚ã‚‹ãŸã‚+1
        else:
            return 0
    except:
        return 0

def apply_mapping(df, mapping):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨"""
    mapped_df = pd.DataFrame()
    
    for system_field, csv_column in mapping.items():
        if csv_column and csv_column in df.columns:
            mapped_df[system_field] = df[csv_column]
    
    return mapped_df

def read_csv_with_encoding(uploaded_file):
    """è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    # æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®å„ªå…ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é †åºï¼ˆShift_JISç³»ã‚’æœ€å„ªå…ˆï¼‰
    encodings = ['shift_jis', 'cp932', 'utf-8', 'utf-8-sig', 'euc-jp', 'iso-2022-jp', 'latin1']
    
    # ã¾ãšæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤åˆ¥
    raw_data = uploaded_file.getvalue()
    detected = chardet.detect(raw_data)
    detected_encoding = detected.get('encoding', '').lower() if detected.get('encoding') else ''
    confidence = detected.get('confidence', 0)
    
    encoding_info = f"ğŸ” æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ¤åˆ¥çµæœ: {detected_encoding.upper() if detected_encoding else 'Unknown'} (ä¿¡é ¼åº¦: {confidence:.2f})"
    
    # MacRomanã‚„ä½ä¿¡é ¼åº¦ã®å ´åˆã¯ç„¡è¦–ã—ã¦Shift_JISã‚’å¼·åˆ¶çš„ã«æœ€åˆã«è©¦è¡Œ
    if detected_encoding == 'macroman':
        # MacRomanã¯æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ä¿¡é ¼ã§ããªã„ - å®Œå…¨ã«ç„¡è¦–
        pass  # Shift_JISã‚’æœ€å„ªå…ˆã§è©¦è¡Œ
    elif confidence < 0.5:
        # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã‚‚Shift_JISã‚’å„ªå…ˆ
        pass  # Shift_JISã‚’æœ€å„ªå…ˆã§è©¦è¡Œ
    elif detected_encoding and detected_encoding not in [enc.lower() for enc in encodings]:
        # ä¿¡é ¼åº¦ãŒé«˜ã„å ´åˆã®ã¿ã€ãã®ä»–ã®åˆ¤åˆ¥çµæœã‚’è©¦è¡Œãƒªã‚¹ãƒˆã«è¿½åŠ 
        encodings.insert(0, detected_encoding)
    
    # å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é †ç•ªã«è©¦è¡Œ
    last_error = None
    best_result = None
    best_score = -1
    encoding_results = []  # ãƒ‡ãƒãƒƒã‚°ç”¨
    
    for encoding in encodings:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
            uploaded_file.seek(0)
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—ã¨ã‚¯ã‚©ãƒ¼ãƒˆæ–‡å­—ã‚’è‡ªå‹•åˆ¤åˆ¥ï¼‰
            df = read_csv_with_options(uploaded_file, encoding)
            
            # èª­ã¿è¾¼ã¿å¾Œã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            quality_score = calculate_japanese_quality_score(df)
            encoding_results.append(f"{encoding}: {quality_score}/10")
            
            if quality_score > best_score:
                best_result = (df, encoding, quality_score)
                best_score = quality_score
            
            # é«˜å“è³ªãªçµæœãŒå¾—ã‚‰ã‚ŒãŸå ´åˆã¯å³åº§ã«æ¡ç”¨
            if quality_score >= 7:  # 10ç‚¹æº€ç‚¹ä¸­7ç‚¹ä»¥ä¸Š
                break
                
        except (UnicodeDecodeError, UnicodeError, LookupError) as e:
            encoding_results.append(f"{encoding}: ã‚¨ãƒ©ãƒ¼({type(e).__name__})")
            last_error = e
            continue
        except Exception as e:
            encoding_results.append(f"{encoding}: ã‚¨ãƒ©ãƒ¼({type(e).__name__})")
            last_error = e
            continue
    
    # æœ€è‰¯ã®çµæœã‚’æ¡ç”¨
    if best_result and best_score >= 2:  # æœ€ä½é™ã®å“è³ªã‚’æº€ãŸã™å ´åˆ
        df, successful_encoding, score = best_result
        success_info = f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{successful_encoding.upper()}' ã§èª­ã¿è¾¼ã¿æˆåŠŸ (å“è³ªã‚¹ã‚³ã‚¢: {score}/10)"
        debug_info = f"ğŸ”§ è©¦è¡Œçµæœ: {' | '.join(encoding_results)}"
        return df, f"{encoding_info}\n{success_info}\n{debug_info}"
    
    # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆ
    if last_error:
        raise Exception(f"ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {str(last_error)}")
    else:
        raise Exception("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

def read_csv_with_options(uploaded_file, encoding):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©åˆ‡ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§èª­ã¿è¾¼ã¿"""
    # åŒºåˆ‡ã‚Šæ–‡å­—ã®å€™è£œ
    separators = [',', ';', '\t', '|']
    
    # ã‚¯ã‚©ãƒ¼ãƒˆæ–‡å­—ã®å€™è£œ
    quote_chars = ['"', "'", None]
    
    best_df = None
    best_columns = 0
    
    for sep in separators:
        for quote_char in quote_chars:
            try:
                uploaded_file.seek(0)
                if quote_char is None:
                    df = pd.read_csv(uploaded_file, encoding=encoding, sep=sep, quoting=3)  # QUOTE_NONE
                else:
                    df = pd.read_csv(uploaded_file, encoding=encoding, sep=sep, quotechar=quote_char)
                
                # ã‚ˆã‚Šå¤šãã®ã‚«ãƒ©ãƒ ã‚’æŒã¤çµæœã‚’å„ªå…ˆ
                if len(df.columns) > best_columns and not df.empty:
                    best_df = df
                    best_columns = len(df.columns)
                    
                # ååˆ†ãªæ•°ã®ã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯æ—©æœŸçµ‚äº†
                if best_columns >= 5:
                    break
                    
            except Exception:
                continue
                
        if best_columns >= 5:
            break
    
    # æœ€è‰¯ã®çµæœã‚’è¿”ã™ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å†è©¦è¡Œ
    if best_df is not None:
        return best_df
    else:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, encoding=encoding)

def calculate_japanese_quality_score(df):
    """èª­ã¿è¾¼ã‚“ã CSVã®å“è³ªã‚’0-10ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡"""
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
        if df.empty or len(df.columns) == 0:
            return 0
        
        score = 0
        
        # ã‚«ãƒ©ãƒ åã¨æœ€åˆã®æ•°è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦æ¤œæŸ»
        sample_texts = []
        
        # ã‚«ãƒ©ãƒ åã‚’ãƒã‚§ãƒƒã‚¯
        column_names = [str(col) for col in df.columns[:10]]
        sample_texts.extend(column_names)
        
        # æœ€åˆã®æ•°è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
        for i in range(min(3, len(df))):
            for j in range(min(5, len(df.columns))):
                sample_texts.append(str(df.iloc[i, j]))
        
        sample_text = ' '.join(sample_texts)
        
        # 1. æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º (ãƒã‚¤ãƒŠã‚¹ç‚¹)
        garbled_patterns = [
            ('ï¿½ï¿½', -5),  # ã‚ˆãã‚ã‚‹æ–‡å­—åŒ–ã‘è¨˜å·
            ('ï¿½ï¿½ï¿½ï¿½', -5),
            ('Ã¯Â¿Â½', -5),
            ('\ufffd', -5),  # Unicode replacement character
            ('ÃƒÂ¤', -3),    # UTF-8ã®æ–‡å­—åŒ–ã‘
            ('ÃƒÂ¯', -3),
            ('ÃƒÂ¦', -3),
            ('Ã¢â‚¬', -3),
            ('Ã£Â¤', -3),    # è¿½åŠ ã®æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³
            ('Ã£Â¯', -3),
            ('Ã£Â¦', -3),
            ('Ã£Â¨', -3),
            ('Ã£â€š', -3),
            ('Ã£â€', -3),
            ('Ã£â€ ', -3),
            ('Ã£â€¦', -3),
            ('Ãª', -2),     # MacRomanç”±æ¥ã®æ–‡å­—åŒ–ã‘
            ('Ã«', -2),
            ('Ã¨', -2),
            ('Ã©', -2),
        ]
        
        for pattern, penalty in garbled_patterns:
            if pattern in sample_text:
                score += penalty
        
        # 2. æ—¥æœ¬èªæ–‡å­—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ (ãƒ—ãƒ©ã‚¹ç‚¹)
        if has_japanese_characters(sample_text):
            score += 5
            
            # ã‚ˆã‚Šè©³ç´°ãªæ—¥æœ¬èªæ–‡å­—ãƒã‚§ãƒƒã‚¯
            import re
            hiragana_count = len(re.findall(r'[\u3040-\u309F]', sample_text))
            katakana_count = len(re.findall(r'[\u30A0-\u30FF]', sample_text))
            kanji_count = len(re.findall(r'[\u4E00-\u9FAF]', sample_text))
            
            # æ—¥æœ¬èªæ–‡å­—ã®ç¨®é¡ãŒå¤šã„ã»ã©é«˜å¾—ç‚¹
            if hiragana_count > 0:
                score += 1
            if katakana_count > 0:
                score += 1
            if kanji_count > 0:
                score += 2
        
        # 3. æ„å‘³ã®ã‚ã‚‹æ–‡å­—åˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æœ‰ã®å†…å®¹ã‚‚å«ã‚€ï¼‰
        meaningful_patterns = [
            'ã‚³ãƒ¼ãƒ‰', 'ãƒ‡ãƒ¼ã‚¿', 'å®Ÿç¸¾', 'äºˆæ¸¬', 'è¨ˆç”»', 'åˆ†é¡', 'å¹´æœˆ',
            'å•†å“', 'å£²ä¸Š', 'éœ€è¦', 'åœ¨åº«', 'ä¾¡æ ¼', 'é‡‘é¡', 'æ•°é‡',
            'ç”Ÿç”£å·¥å ´', 'ç”Ÿç”£ãƒ©ã‚¤ãƒ³', 'ABCåŒºåˆ†', 'å‡ºåº«å®Ÿç¸¾', 'ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰',
            'æ§‹æˆæ¯”', 'ç•°å¸¸å€¤', 'é ˆè³€å·'  # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æœ‰ã®å†…å®¹
        ]
        
        matched_patterns = 0
        for pattern in meaningful_patterns:
            if pattern in sample_text:
                matched_patterns += 1
        
        # ãƒãƒƒãƒã—ãŸæ„å‘³ã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ•°ã«å¿œã˜ã¦ã‚¹ã‚³ã‚¢ã‚’åŠ ç®—
        if matched_patterns >= 3:
            score += 3  # å¤šãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãƒãƒƒãƒã—ãŸå ´åˆã¯é«˜å¾—ç‚¹
        elif matched_patterns >= 1:
            score += 1
        
        # 4. åŸºæœ¬çš„ãªèª­ã¿è¾¼ã¿æˆåŠŸãƒœãƒ¼ãƒŠã‚¹
        score += 2
        
        # ã‚¹ã‚³ã‚¢ã‚’0-10ã®ç¯„å›²ã«æ­£è¦åŒ–
        return max(0, min(10, score))
        
    except Exception:
        return 0

def has_japanese_characters(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    import re
    # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã®Unicodeç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯
    japanese_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]')
    return bool(japanese_pattern.search(text))

def validate_mapped_data(df):
    """ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
    try:
        # å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªï¼ˆABCåŒºåˆ†ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãŸã‚é™¤å¤–ï¼‰
        required_cols = ['P_code', 'Date', 'Actual', 'AI_pred', 'Plan_01']
        for col in required_cols:
            if col not in df.columns:
                st.error(f"å¿…é ˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {col}")
                return False
        
        # ABCåŒºåˆ†ã®å­˜åœ¨ç¢ºèªï¼ˆå¿…é ˆï¼‰
        if 'Class_abc' not in df.columns:
            st.error("ABCåŒºåˆ†ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        numeric_cols = ['Actual', 'AI_pred', 'Plan_01']
        if 'Plan_02' in df.columns:
            numeric_cols.append('Plan_02')
            
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            if df[col].isnull().any():
                st.warning(f"âš ï¸ {col}ã«æ•°å€¤ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼ˆNaNã«å¤‰æ›ã•ã‚Œã¾ã—ãŸï¼‰")
        
        # æ—¥ä»˜å½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰
        try:
            pd.to_datetime(df['Date'].astype(str), format='%Y%m')
        except:
            st.warning("âš ï¸ Dateåˆ—ã®å½¢å¼ãŒYYYYMMã§ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        return True
        
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False 